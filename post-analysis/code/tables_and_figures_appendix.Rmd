---
title: "Appendix"
subtitle: "Cross-National Differences in Market Response: Line-Length, Price, and Distribution Elasticities in Fourteen Indo-Pacific Rim Economies"
output:
  html_document: default
date: "`r format(Sys.time(), '%d %B %Y')`"
---

```{r setup, include=FALSE}
# knitr options
knitr::opts_chunk$set(echo = TRUE)
options(xtable.comment = FALSE)
options(knitr.kable.NA = '')

# load packages
library(data.table)
library(stargazer)
library(knitr)
library(xtable)
library(car)
library(kableExtra)
library(stringi)
library(ggplot2)
library(lmtest)
library(sandwich)
library(gridExtra)
library(ggthemes)

# Load data set
brand_panel=fread('../../analysis/temp/preclean_main.csv')
brand_panel[, ':=' (date = as.Date(date))]

# Load auxiliary functions
source('proc_auxilary.R')
source('proc_rename.R')

# Load results
load('../output/workspace.RData')

predictions <- fread('../output/predictions_within.csv')
predictions_kfold = fread('../output/predictions_kfold.csv')

# set order of variables to appear in figures and tables
ordered_vars =  c('llen', 'rwpspr', 'wpswdst')
ordered_vars = ordered_vars[which(ordered_vars%in%elast$variable)]

names(ordered_vars) <- paste0(unlist(sanitize_table(data.frame(gsub('^ln', '', ordered_vars)))), ' elasticity')

# Notes for tables
notes_sig = 'Significance levels: \\* *p*<.1, \\*\\* *p*<.05, \\*\\*\\* *p*<.01 (two-sided).'
estimnote = paste0('Elasticities are weighted by inverse standard errors.')

# Significance levels
sigvalue = .1
zval = qnorm(1-sigvalue/2)

```


```{r reset, echo=FALSE,results='asis'}
tableno<<-0
figureno<<-0
cat("<P style='page-break-before: always'>")
  
```


```{r sample_origin, echo=FALSE, results='asis'}

cols =  c('country_of_origin')

 
tmp = elast[, list(.N), by = c('brand',cols)]
tmp = tmp[!grepl('alloth', brand)]
tmp[brand=='threed', brand:='3D']
tmp <- tmp[!grepl('super|amazon', brand)]
tmp = tmp[, list(nbrands=length(unique(brand)), all_brands=paste(my_capitalize(brand)[order(brand)], collapse=', ')), by = cols]
tmp[country_of_origin==''|is.na(country_of_origin), country_of_origin := 'Country not available']
tmp[, country_of_origin:=my_capitalize(country_of_origin)]

setorderv(tmp, 'nbrands', order=-1L)
setcolorder(tmp, c('country_of_origin', 'nbrands', 'all_brands'))

tmp=sanitize_table(tmp)

kable(tmp, caption = tab('Country-of-origins for brands in sample^1^', prefix ='A'),format='html') %>%
        kable_styling() %>% footnote(number=paste0('Countries are ordered by the number of brands from a given country; brand names are listed alphabetically.'))

sum(tmp$`Number of brands`)

```

```{r overview2, echo=FALSE, warning=FALSE, results='asis'}
#elast_all
tmp = elast[, list(Nbrands = length(unique(brand))), by = c('category','country')]

tabl = dcast.data.table(tmp, category~country, value.var=c('Nbrands'))
rownames(tabl) <- NULL

tabl=sanitize_table(tabl)
tabl[, sort_category:=tolower(`Category`)]
setorder(tabl, sort_category)
tabl[, sort_category:=NULL]

kable(tabl, caption = tab('Overview of markets (category/country combination) in the sample (with number of selected brands used in model estimation indicated per cell^1^)', prefix='A'))%>% kable_styling() %>% footnote(number=paste0('Number of brands include the composite brand, which aggregates all brands with market shares lower than 1% in 5 consecutive years (4 years for tablets).'))

```


```{r summarystats_model, echo=FALSE, results= 'asis'}

covars_summary <- gsub('ln', '', c('usales', gsub('rwpspr', 'rwpsprd', ordered_vars)))

tmp=data.table(brand_panel)
nbrands=length(unique(brand_panel$brand))
nmarkets=length(unique(brand_panel$market_id))
nobs=nrow(tmp)

tmp=tmp[, lapply(.SD, function(x) c(median=median(x,na.rm=T),firstqnt=quantile(x,.25,na.rm=T),thirdqnt=quantile(x,.75,na.rm=T))), .SDcols=c(covars_summary)]


tmp[, var:=rep(c('median', 'firstqnt','thirdqnt'),1)]
tmp=melt(tmp, id.var=c('var'))

dtf=dcast(tmp, variable~var)
setcolorder(dtf, c('variable', rep(c('median', 'firstqnt','thirdqnt'),1)))

### add correlation
give_cor <- function(cordat=data.frame(brand_panel[, covars_summary,with=F])) {
  correl=cor(cordat,use='pairwise.complete')
  N=matrix(double(prod(dim(correl))),ncol=ncol(cordat))
  t=N
  p=N
  for (i in 1:nrow(correl)) {
    for (j in 1:ncol(correl)) {
      N[i,j]=length(which(complete.cases(cordat[,c(i,j)])))
      t[i,j]=sqrt(N[i,j]-2)*(correl[i,j]/sqrt(1-correl[i,j]^2))
      p[i,j]=dt(t[i,j], df=N[i,j]-2)
    }
  }
return(list(cor=correl,p=p))
}

correl = give_cor(data.frame(brand_panel[, covars_summary,with=F]))

dtf=cbind(dtf,correl$cor[,-ncol(correl$cor)])


dtf=sanitize_table(dtf)


print(kable(dtf, format='html', initial.zero = FALSE, digits=c(1,0,0,0,3,3,3),caption = tab(paste0('Summary statistics and correlations for variables in sales response model^1^'),prefix='A')) %>% kable_styling() %>% footnote(number=c(paste0('Summary statistics and correlations for variables (prior to the log-operation and mean-centering) are computed across ', prettyNum(nobs, big.mark = ','), ' observations in our sample.'),'Converted to USD.')))
      #
    #%>% add_header_above(c(" " = 1, "Summary statistics" = 3, "Correlations" = length(covars_summary)-1)))

```


```{r, result = 'asis'}

# Correlations for equity (standardized within category)

rowvars = c('sbbe_round1_mc','brand_from_jp-us-ch-ge-sw',
            'ln_llen_windex_mc', 'ln_rwpspr_windex_mc' ,
            'ln_wpswdst_windex_mc',
             'ln_nov6sh_windex_mc',
            'ln_market_herf_mc', 'ln_market_meangrowth_mc',
             'appliance',
            'ln_penn_popyravg_mc', 'ln_penn_percapitargdpeyravg_mc',
            'ln_penn_growthrgdpeyravg_mc', 'ln_ginicoef_mc',
             'ln_pdi_mc' ,'ln_uai_mc','ln_mas_mc')


tmp = unique(elast, by = c('category','country','brand'))[, c(rowvars),with=F]
tmp = tmp[complete.cases(tmp)]

tmpcor=cor(as.matrix(tmp))

options(width=600)
sink('../output/correlations_secondstage.txt')
print(corstars(as.matrix(tmp), method="pearson", removeTriangle='none', ndec=3))
cat(paste0('\nNumber of observations: ', nrow(as.matrix(tmp)), '\n'))
sink()

write.table(tmpcor, '../output/correlations_secondstage.csv', row.names=T)

print(corstars(as.matrix(tmp), method="pearson", removeTriangle='none', ndec=3, result = 'html'))



```



```{r attributes, echo = FALSE, results= 'asis'}

# for each category

attributes <- suppressWarnings(melt(brand_panel[!is.na(usales), c('category', grep('^attr', colnames(brand_panel),value=T)),with=F], id.vars=c('category')))

attributes[, na:=all(is.na(value)),by=c('category','variable')]


# subtract 1 if max is 101 and min is 1
attributes[, conversion_needed:=min(value,na.rm=T)==1&max(value,na.rm=T)==101,by=c('variable')]
attributes[variable%in%c('attr_digitalzoom', 'attr_3dyes'), conversion_needed:=T]
attributes[conversion_needed==T, value:=value-1]


attr = attributes[na==F, list(nobs = .N, meancap=mean(value), sdcap=sd(value), mincap=min(value), maxcap=max(value)),by=c('category','variable')]
setorder(attr,category,variable)
.vars=c('nobs','meancap','sdcap', 'mincap', 'maxcap')
for (.var in .vars) attr[, (.var):=as.character(formatC(get(.var), big.mark=',',digits=ifelse(.var=='nobs', 0, 3), format = 'f'))]

attr<-attr[!grepl('freezernotestimable', variable)]
setnames(attr, 'variable','Attribute')
tabl=sanitize_table(attr)
tabl[, sort_category:=tolower(`Category`)]
tabl[, sort_attribute:=tolower(`Attribute`)]

setorder(tabl, sort_category, sort_attribute)
tabl[, sort_category:=NULL]
tabl[, sort_attribute:=NULL]


print(kable(tabl, digits=3, caption = tab('Overview of physical search attributes per category^1^', prefix='A')) %>% kable_styling() %>% footnote(number=c(paste0('The unit of analysis is the brand-month level. Indicator variables are either 0 or 1 at the SKU-month-level; when aggregating them to the brand-month level for the analysis, they are averaged and hence measure the share of a brand\'s SKUs that carry a particular product attribute.'))))
```


<P style='page-break-before: always'>



```{r elasticities_comparison, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}

unitlist = list(
   list(
    units = c(
      'Advertising not included (focal model)' = 'ec_chinahk_withoutadv_sur',
      'Advertising included' = 'ec_chinahk_withadv_sur'
    ),
    title = 'Differences in Long-Term Elasticities in Models With and Without Advertising Spending^1^'
  ),
  
  list(
    units = c(
      'Estimated without product attributes (main model)' = 'ec_main_sur',
      'Estimated with product attributes' = 'ec_main_attributes_sur'
    ),
    title =  'Differences With And Without Product Attributes^1^'
  ),
  
     list(
    units = c(
      'Without Long-Run Competitive Effects (main model)' = 'ec_main_sur',
      'With Long-Run Competitive Effects' = 'ec_unrestrictedcompetition_sur'
    ),
    title =  'Differences With And Without Long-Run Competitive Effects^1^'
  ),
  
  list(
    units = c(
      'Estimated on split data set (early)' = 'ec_first60_sur',
      'Estimated on split data set (late)' = 'ec_last60_sur'
    ),
    title =  'Differences in Long-Term Elasticities Over Time^1^'
  ))

for (u in unitlist) {
  units = u$units
  compare_elast = rbindlist(lapply(units, function(.unit) {
    return(data.table(elasticities[[.unit]])[, c('category',
                                     'country',
                                     'brand',
                                     'variable',
                                     'elastlt',
                                     'elastlt_se'), with = F][, ':=' (
                                       type = .unit,
                                       w_elastlt = 1 / elastlt_se,
                                       elastlt_z = elastlt / elastlt_se
                                     )])
  }))
  
  
  # keep only markets for which both estimates are available
  if (any(grepl('adv', units))) {
    compare_elast[, has_adv := any(grepl('adv', variable)), by = c('category', 'country', 'brand')]
    compare_elast <- compare_elast[has_adv == T]
  } else {
    compare_elast[, N := .N, by = c('category', 'country', 'brand', 'variable')]
  compare_elast <- compare_elast[N == 2][, N := NULL]
  
  }
  
  
  compare_elast[, ttest_indic := ifelse(type == units[2], T, F)]
  
  
  
  tmp = compare_elast[, list(
    N = .N,
    mean = sum(elastlt * w_elastlt) / sum(w_elastlt),
    rosenthal = signstars(sum(elastlt_z) / sqrt(length(which(
      !is.na(elastlt)
    ))))
  ), by = c('type', 'variable')]
  setnames(tmp, 'variable', 'vars')
  
  
  # t-tests
  ttests = rbindlist(lapply(grep(
    'adv',
    unique(tmp$vars),
    invert = T,
    value = T
  ), function(.v) {
    tmp2 = summary(lm(
      elastlt ~ 1 + ttest_indic,
      data = compare_elast,
      subset = variable == .v
    ))
    tmp3 = data.table(variable = .v, rbind(tmp2$coefficients[2, ]))
    setnames(tmp3, c('vars', 'est', 'se', 't', 'p'))
  }))
  
  tmp[, type := factor(as.character(type), levels = units)]
  
  
  tmpx = data.table(dcast(melt(tmp, id.vars = c('type', 'vars')), vars ~ type + variable))
  
  tmpx[, order := match(vars, c('radv', ordered_vars))]
  
  tmpx = merge(tmpx,
               ttests[, c('vars', 't', 'p')],
               by.x = 'vars',
               by.y = 'vars',
               all.x = T)
  setorder(tmpx, order)
  tmpx[, order := NULL]
  for (.v in grep('N$|mean$|est$|se$|t$|p$', colnames(tmpx), value = T))
    tmpx[, paste0(.v) := as.numeric(get(.v))]
  
  la = sanitize_table(tmpx)
  nheaders = c(' ', names(units), 'Tests on differences')
  headers = c(1, 3, 3, 2)
  names(headers) = nheaders
  
  
  print(
    kable(
      la,
      digits = 3,
      format = 'html',
      format.args = list(big.mark = ","),
      caption = tab(paste0(u$title), prefix = 'A')
    ) %>% kable_styling() %>% add_header_above(headers)
  )
}

```




```{r withinR2, results='asis', echo = FALSE, include = TRUE}


ord=c('salesresponse_linear',
      'ec_lntrend',
      'ec_main')

sel_ord = ord


#predictions

estim_size = predictions[, list(N=length(which(estim_set==T))),by=c('type', 'category','country','brand')]
setkey(estim_size, category,country,brand,type)

#predictions <- predictions[]

predictions[, ':=' (common_dv=as.numeric(NA),
                    common_dv_hat = as.numeric(NA))]

#predictions[type=='ec_log']

predictions[grepl('^ec', type)&grepl('log',type), ':=' (common_dv = exp(lnusales),
                                                        common_dv_hat = exp(lnusales_hat))]
predictions[grepl('^ec', type)&!grepl('log',type), ':=' (common_dv = usales,
                                                        common_dv_hat = usales_hat)]
predictions[grepl('^salesresponse', type)&grepl('log',type), ':=' (common_dv = exp(lnusales),
                                                        common_dv_hat = exp(dv_hat))]
predictions[grepl('^salesresponse', type)&!grepl('log',type), ':=' (common_dv = usales,
                                                        common_dv_hat = dv_hat)]



table(predictions$type)


#r2s[type=='ec_main'&brand=='hp'&category=='laptop'&country=='hong kong']


# how to compare?
r2s=predictions[type%in%sel_ord, list(#R2_diff = cor(dlnusales, dlnusales_hat, use='pairwise')^2,
                       R2_lev = cor(common_dv, common_dv_hat, use='pairwise')^2,
                       #cor_diff = cor(dlnusales, dlnusales_hat, use='pairwise'),
                     # cor_lev = cor(lnusales, lnusales_hat, use='pairwise'),
                      # MSE_diff = mean((dlnusales_hat-dlnusales)^2,na.rm=T),
                       RMSE_lev = sqrt(mean((common_dv_hat-common_dv)^2,na.rm=T)),
                     MeAPE_lev = median(abs((common_dv-common_dv_hat)/common_dv),na.rm=T),
                     SMAPE_lev = mean((abs(common_dv_hat-common_dv))/(.5*(abs(common_dv)+abs(common_dv_hat))),na.rm=T),
                     N=length(which(!is.na(common_dv_hat)))),
             by = c('type','category','country','brand')]

 setkey(r2s, category, country, brand, type)
  r2s[estim_size, N:=i.N]
  

  tmp = r2s[, lapply(.SD, mean,na.rm=T), by = c('type'), .SDcols=grep("(R2|RMSE|MeAPE|SMAPE).*lev$",colnames(r2s),value=T)]
  
  # --> investigate
  
  tmp[, ord:=match(type, ord)]
  
  setorder(tmp, ord)
  tmp[, ord:=NULL]
  lbl = 'Model selection (within model fit)'
  tmp2=tmp
  tmp2 = sanitize_table(tmp)
  setnames(tmp2, 'R2_lev', 'R2')
  setnames(tmp2, 'RMSE_lev', 'RMSE')
  setnames(tmp2, 'MeAPE_lev', 'MeAPE')
  setnames(tmp2, 'SMAPE_lev', 'SMAPE')
  
  
  #setnames(tmp2, 'type', 'Model type')
  print(kable(tmp2, format= 'html', caption =lbl, digits = 5) %>% kable_styling()%>% footnote(number=paste0('Fit metrics computed across the ', nrow(unique(predictions, by =c('category','brand','country'))), ' estimated models, using predicted and observed sales in levels. R2 is the average squared correlation. RMSE is average root mean squared error. MeAPE is the median absolute percentage error. SMAPE is the symmetric mean absolute percentage error (Armstrong and Collopy 1992).')))

```

```{r holdoutrsq, results='asis', echo = FALSE, include= TRUE}

tmppred = predictions_kfold[type%in%ord]

setorder(tmppred, type, category,country,brand,kfold,date)
tmppred[, type:=as.factor(type)]

tmppred[, ':=' (common_dv=as.numeric(NA),
                    common_dv_hat = as.numeric(NA))]

table(tmppred$type)

tmppred[grepl('^ec', type)&grepl('log',type), ':=' (common_dv = exp(lnusales),
                                                        common_dv_hat = exp(ldv+ddv_hat))]


tmppred[grepl('^ec', type)&!grepl('log',type), ':=' (common_dv = usales,
                                                        common_dv_hat = ldv + ddv_hat)]

tmppred[grepl('^salesresponse', type)&grepl('log',type), ':=' (common_dv = exp(lnusales),
                                                        common_dv_hat = exp(dv_hat))]
tmppred[grepl('^salesresponse', type)&!grepl('log',type), ':=' (common_dv = usales,
                                                        common_dv_hat = dv_hat)]

# measures defined for all models?
summary(tmppred$common_dv)

tmp = tmppred[, list(R2 = cor(common_dv, common_dv_hat, use='pairwise')^2,
                         RMSE = sqrt(mean((common_dv-common_dv_hat)^2,na.rm=T)),
                     MeAPE = median(abs((common_dv-common_dv_hat)/common_dv),na.rm=T),
                     SMAPE_lev = mean((abs(common_dv_hat-common_dv))/(.5*(abs(common_dv)+abs(common_dv_hat))),na.rm=T),
                     N=length(which(!is.na(common_dv_hat)))),
                  by=c('type','category','country','brand','kfold', 'estim_set')]


# ADD TRAINING AND PREDICTION TO SET
# correct predictions
sel_ord=ord
tmp2=tmp[type%in%sel_ord, lapply(.SD, mean), by = c('type', 'estim_set'), .SDcols=grep("R2|mse|mape", colnames(tmp),value=T, ignore.case=T)]
tmp2[, type2:=factor(ifelse(estim_set==T, 'training','holdout'), levels=c('training','holdout'))]
tmp3 = dcast(melt(tmp2, id.vars=c('type','type2', 'estim_set')), type~variable+type2)

tmp3[, ord:=match(type, ord)]
  
setorder(tmp3, ord)
tmp3[, ord:=NULL]

lbl = 'Model selection (hold-out sample)'
  
  tmp4 = sanitize_table(tmp3)
 # setnames(tmp2, 'R2_lev', 'R2')
 # setnames(tmp2, 'MSE_lev', 'MSE')
  
  setnames(tmp4, 'type', 'Model type')
  print(kable(tmp4, format= 'html', caption =lbl, digits = 5) %>% add_header_above(list(' '=1, 'R-squared' =2, 'RMSE'=2, 'SMAPE'=2)))
  


```


