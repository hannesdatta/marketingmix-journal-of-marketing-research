---
title: "Simulation of Latent Instrumental Variables"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r, echo = FALSE}
set.seed(1234)
```

As a first step towards our own LIV estimation for the GfK project, I propose to first recap MLE (see 1), before extending it to LIV (see 2).

References/Notation:

Ebbes, P., Wedel, M., BÃ¶ckenholt, U., & Steerneman, T. (2005). Solving and testing for regressor-error (in) dependence when no instrumental variables are available: With new evidence for the effect of education on income. *Quantitative Marketing and Economics*, 3(4), 365-392. To the extent possible, I adhere to the notation in that paper.

# 1.0 Two regressions w/ correlated errors (not LIV)

As a first step, I am trying to conduct MLE on two regressions with correlated errors

# 1.1 Simulate data

```{r}

sim_data <- function(rho = .5) {
  # Number of observations
  N = 1000
  
  # Coefficients on latent instruments
  pi = c(5, 2) 
  
  # Class probabilities
  p = c(.7, .3)
  
  # Assert the probs sum to 1
  stopifnot(sum(p)==1)
  
  # Number of latent instruments
  k = length(pi)
  
  # Correlation matrix
  R = matrix(c(1, rho,
                  rho, 1), ncol=2)
  # Standard deviations
  S <- c(2, 1)
  
  cor2cov <- function(R, S) {
   sweep(sweep(R, 1, S, "*"), 2, S, "*")
  }
  
  Sigma = cor2cov(R, S)
  
  # Draws from the Var-Covar Matrix
  
  #sdcor2cov
  library(MASS)
  set.seed(1234)
  errorterm = mvrnorm(n = N, mu = c(0,0), Sigma =Sigma)
  
  cor(errorterm)
  cov(errorterm)
  
  z <- sample(1:k, N, replace = T, prob = p)
  cbind(class = 1:k, frequency = table(z), prob = table(z)/sum(table(z)))
  x2 = rnorm(N)
  
  # Simulate data
  x = 5 + pi[z] + errorterm[,2]
  
  y = 1 + 3 * x2 + errorterm[,1]
  
  return(list(data = data.frame(y, x, x2, pi=pi[z]), ols = lm(y~1+x)))

}
```

## 1.2 Estimate using OLS

```{r}

out=sim_data(rho = .5)
summary(lm(x~1+pi, data = out$data))
summary(lm(y~1+x2, data = out$data))
```

# 1.3 Recovery using MLE
```{r}
library(mvtnorm)

llik <- function(par) {
  
  beta0 = par[1]
  beta1 = par[2]
  beta2 = par[3]
  beta3 = par[4]
  
  lchol = diag(2)
  lchol[1,1] <- par[5]
  lchol[2,1] <- par[6]
  lchol[2,2] <- par[7]
  
  sigma <- crossprod(lchol)
  
  #sigma <- diag(2)
  
	#lambda1 = exp(par[6])/(1+exp(par[6]))
	#lambda = c(lambda1, 1-lambda1)
	
	#pi = par[7:8]
	
	x_pred = beta0+beta1*pi
	y_pred = beta2 + beta3 * x2
	y_mu = beta0 + beta1 * x
	  
	# the likelihood of y
	-sum(dmvnorm(cbind(y-y_pred,x-x_pred), mean = c(0, 0), sigma = sigma, log=TRUE))
	
	#-sum(dnorm(y-y_mu, mean=0, sd=sigma[1,1], log=TRUE))
  
}


library(stats4)
out=sim_data(rho = .5)
summary(out$ols)

y=out$data$y
x=out$data$x
x2=out$data$x2
pi=out$data$pi

MLE = optim(rep(0,7), fn = llik,
            method = 'L-BFGS-B',
            lower = .000001)
MLE$par

lchol = diag(2)
lchol[1,1] <- MLE$par[5]
lchol[2,1] <- MLE$par[6]
lchol[2,2] <- MLE$par[7]
sigma <- crossprod(lchol)  
#covariance
sigma
# correlation
cov2cor(sigma)

# perfect recovery!

# ...so, it's more about simulation of category memberships than it is about recovery
#summary(ols)


```

# 2.0 LIV 

# 2.1 Simulate data

```{r}

sim_data <- function(rho = .5, N = 1000) {
  # Number of observations
  #N = 1000
  
  # Coefficients on latent instruments
  pi = c(5, 7) 
  
  # Class probabilities
  p = c(.7, .3)
  
  # Assert the probs sum to 1
  stopifnot(sum(p)==1)
  
  # Number of latent instruments
  k = length(pi)
  
  # Correlation matrix
  R = matrix(c(1, rho,
                  rho, 1), ncol=2)
  # Standard deviations
  S <- c(2, 1)
  
  cor2cov <- function(R, S) {
   sweep(sweep(R, 1, S, "*"), 2, S, "*")
  }
  
  Sigma = cor2cov(R, S)
  
  # Draws from the Var-Covar Matrix
  
  library(MASS)
  set.seed(1234)
  errorterm = mvrnorm(n = N, mu = c(0,0), Sigma =Sigma)
  
  cor(errorterm)
  cov(errorterm)
  
  z <- sample(1:k, N, replace = T, prob = p)
  cbind(class = 1:k, frequency = table(z), prob = table(z)/sum(table(z)))
  
  # Simulate data
  x = pi[z] + errorterm[,2]
  
  y = 1 + 3 * x + errorterm[,1]
  
  return(list(data = data.frame(y, x)))

}
```

## 2.2 Estimate using OLS

```{r}

# bias:
out=sim_data(rho = .5)
summary(lm(y~1+x, data = out$data))

# no bias:
out=sim_data(rho = 0)
summary(lm(y~1+x, data = out$data))

```

# 2.3 Recovery using MLE a la Ebbes et al. 
```{r}

library(mvtnorm)

set.seed(999)
out=sim_data(rho = .5)

N = nrow(out$data)

pars <- c(1,3,1,1,1, .5, 5, log(2))

y = out$data$y
x= out$data$x

map_pars <- function(pars) {
  beta0 = pars[1]
  beta1 = pars[2]
  
  # Covariance/correlation structure
  lchol = diag(2)
  lchol[1,1] <- pars[3]
  lchol[2,1] <- pars[4]
  lchol[2,2] <- pars[5]
  
  sigma <- crossprod(lchol)
  
  # class memberships
  prob <- exp(pars[6])/(1+exp(pars[6]))
  
  # instrument coefficients
  lambdas <- double(2)
  lambdas[1] <- pars[7]
  lambdas[2] <- pars[7] + exp(pars[8])
  
  return(list(beta0=beta0, beta1=beta1, lchol=lchol, sigma=sigma, prob=prob, lambdas=lambdas))
}

llik <- function(pars) {
  
  mapped_pars = map_pars(pars)
  beta0 = mapped_pars$beta0
  beta1 = mapped_pars$beta1
  sigma = mapped_pars$sigma
  prob = mapped_pars$prob
  lambdas = mapped_pars$lambdas
  
  # expected values
  x_pred <- prob * lambdas[1] + (1-prob) * lambdas[2] 
  y_pred = beta0 + beta1 * x_pred

  llik = dmvnorm(cbind(y-y_pred, x-x_pred),mean=c(0,0), sigma=sigma, log=T)
  
  return(-sum(llik))
}


library(stats4)
out=sim_data(rho = .5)

y=out$data$y
x=out$data$x


llik(pars)

# OLS start values
ols1 <- lm(y~1+x)

start_vals = c(drop(ols1$coefficients)[1], drop(ols1$coefficients)[2], 1, 0, 1, 0, 0,0)

MLE = optim(start_vals, fn = llik,
            method = 'L-BFGS-B',
            lower = .000001)

# recovery is not quite there yet...
map_pars(MLE$par)


sigma

# correlation
cov2cor(map_pars(MLE$par)$sigma) # --> extreme high correlation

```
# 2.2 Recovery using MLE w/ simulated max. Lik
```{r}

library(mvtnorm)

r = 1500 # number of random draws

set.seed(999)
out=sim_data(rho = .5)

N = nrow(out$data)
p_draws <- matrix(runif(r*N), ncol=r)

pars <- c(1,3,1,1,1, .5, 5, log(2))

y = out$data$y
x= out$data$x

llik <- function(pars) {
  
  beta0 = pars[1]
  beta1 = pars[2]
  
  # Covariance/correlation structure
  lchol = diag(2)
  lchol[1,1] <- pars[3]
  lchol[2,1] <- pars[4]
  lchol[2,2] <- pars[5]
  
  sigma <- crossprod(lchol)
  #sigma <- diag(1)
  
  # class memberships
  prob <- exp(pars[6])/(1+exp(pars[6]))
  
  # instrument coefficients
  lambdas <- double(2)
  lambdas[1] <- pars[7]
  lambdas[2] <- pars[7] + exp(pars[8])
  
  classes <- ifelse(p_draws<prob, 1,2)
  
  x_pred <- apply(classes, 2, function(x) lambdas[x])
  
  y_pred = beta0 + beta1 * x_pred
 # y_pred = beta0 + beta1 * matrix(rep(x, r),ncol=r)
  
  #x_pred
  
  y_r = matrix(rep(y, r),byrow=F, ncol=r)
  x_r = matrix(rep(x, r),byrow=F, ncol=r)
  
  ps= sapply(1:r, function(l) {
    dmvnorm(cbind(y_r[,l]-y_pred[,l], x_r[,l]-x_pred[,l]),mean=c(0,0), sigma=sigma, log=T)
  })
  
  llik_vals = colSums(ps)
  
  -sum(llik_vals)
  #-log(mean(exp(llik_vals-max(llik_vals))))
  
  #-sum(ps[,1])
  
  #-log(mean(llik_vals))
  #-log(mean(exp(llik_vals-max(llik_vals))))

}


library(stats4)
out=sim_data(rho = .5)

y=out$data$y
x=out$data$x


llik(pars)


MLE = optim(pars, fn = llik,
            method = 'L-BFGS-B',
            lower = .000001)

MLE$par

summary(lm(y~1+x))


lchol = diag(2)
lchol[1,1] <- MLE$par[5]
lchol[2,1] <- MLE$par[6]
lchol[2,2] <- MLE$par[7]
sigma <- crossprod(lchol)  
#covariance
sigma
# correlation
cov2cor(sigma)

```

## 2.1 Try out MLE and compare w/ OLS

```{r}
library(mvtnorm)

llik <- function(par) {
  
  beta0 = par[1]
  beta1 = par[2]
  
  lchol = diag(2)
  lchol[1,1] <- par[3]
  lchol[2,1] <- par[4]
  lchol[2,2] <- par[5]
  
  sigma <- crossprod(lchol)
  
  sigma <- diag(2)
  
	lambda1 = exp(par[6])/(1+exp(par[6]))
	lambda = c(lambda1, 1-lambda1)
	
	pi = par[7:8]
	
	x_mu = sum(lambda * pi)
	y_mu = beta0 + beta1 * x
	  
	# the likelihood of y
	#-sum(dmvnorm(cbind(y-y_mu, x), mean = c(0, x_mu), sigma = sigma, log=TRUE))
	
	-sum(dnorm(y-y_mu, mean=0, sd=sigma[1,1], log=TRUE))
  
}


library(stats4)
out=sim_data(rho = .5)
summary(out$ols)

y=out$data[,'y']
x=out$data[,'x']

MLE = optim(rep(0,8), fn = llik,
            method = 'L-BFGS-B',
            lower = .000001)
MLE

#summary(ols)


```


```{r}
# test 2
neg_log_lik_gaussian <- function(pars) {
  #betas[2]=0
  xsi = 1 #exp(pars[3])/(1+exp(pars[3]))
  
  y_pred = pars[1] + pars[2]*x
  
  ll2 = -sum(dnorm(y - y_pred, mean=0, sd=xsi, log=TRUE))
  
  ll2
}

lm(y~1+x)


library(stats4)

MLE = optim(c(5,0,0), fn = neg_log_lik_gaussian,
            method = 'L-BFGS-B',
            lower = .00001)

MLE$par

```

## 2.2 Draft MLE estimator for LIV
```{r}
set.seed(634123)

p_draws <- runif(N)

neg_log_lik_gaussian_liv <- function(pars) {
  beta = pars[1:3]
  gamma = pars[4]
  xsi = double(2)
  xsi[1] = exp(pars[5])/(1+exp(pars[5]))
  
  alpha = pars[6]
  lambdas = pars[7:8]
  p = exp(pars[9])/(1+exp(pars[9]))
  
  rho = exp(pars[10])/(1+exp(pars[10]))
  
  xsi[2] = exp(pars[11])/(1+exp(pars[11]))
  
  classes <- ifelse(p_draws<p, 1,2)
    
  m_pred = alpha * x + lambdas[classes]
  
  ll1 = -sum(dnorm(m-m_pred, mean=0, sd=xsi[2], log=TRUE))
  
  y_pred = beta[1] + beta[2]*m_pred + beta[3] * z + gamma * x + rho * (m - m_pred)
  
  ll2 = -sum(dnorm(y-y_pred, mean=0, sd=xsi[1], log=TRUE))
  
  ll1 + ll2
}


#

# (1) p1 = TE BEPALEN DOOR OPTIMIZER   (  exp(p)/(1+exp(p)))  --> bounded between 0 and 1
#     p2 = 1- p1

# Binominal logit -

#    b1
#    b2
     b3 --> 0
# -->  exp(b1)/sum(1+ exp(b2) + exp(b3))


# Stel ik heb DRIE CLASSES; TWEE PARAMETERS.


library(stats4)

MLE = optim(c(0,0,0,0,0,0,0,0,0,0,0), fn = neg_log_lik_gaussian_liv,
            method = 'L-BFGS-B',
            lower = .00001)

names(MLE$par) <- c('beta1','beta2','beta3','gamma','xsi1', 'alpha','lambda1','lambda2', 'prob_classmembership', 'rho', 'xsi2')

MLE


```

- Parameters are "off"
- Some parameters do not get estimates
- I am quite confident I simulated the data correctly. I think my LIV likelihood function is incorrect.


## 2.3 Draft MLE estimator for LIV
```{r}

set.seed(634123)

d = 100 # draws

p_draws <- matrix(runif(N*d), ncol=d)


neg_log_lik_gaussian_liv <- function(pars) {
  beta = pars[1:3]
  gamma = pars[4]
  xsi = double(2)
  xsi[1] = exp(pars[5])/(1+exp(pars[5]))
  
  alpha = pars[6]
  lambdas = pars[7:8]
  p = exp(pars[9])/(1+exp(pars[9]))
  
  rho = exp(pars[10])/(1+exp(pars[10]))
  
  xsi[2] = exp(pars[11])/(1+exp(pars[11]))
  
  classes <- ifelse(p_draws<p, 1,2)
    
  m_pred = apply(classes, 2, function(class) alpha * x + lambdas[class])
  m_pred_m = m-m_pred
  
  ll1 = -sum(dnorm(m-m_pred, mean=0, sd=xsi[2], log=TRUE))
  
  y_pred = beta[1] + beta[2]*m_pred + beta[3] * z + gamma * x + rho * (m - m_pred)
  
  ll2 = -sum(dnorm(y-y_pred, mean=0, sd=xsi[1], log=TRUE))
  
  ll1 + ll2
}

library(stats4)

MLE = optim(c(0,0,0,0,0,0,0,0,0,0,0), fn = neg_log_lik_gaussian_liv,
            method = 'L-BFGS-B',
            lower = .00001)

names(MLE$par) <- c('beta1','beta2','beta3','gamma','xsi1', 'alpha','lambda1','lambda2', 'prob_classmembership', 'rho', 'xsi2')

MLE

```

```{r}
# Simulate "bias" for several rhos between 0 and 1.

rhos = seq(from = 0, to = 1, length.out=20)

set.seed(2)
  
estimates = lapply(rhos, function(rho) return(sim_data(rho)$ols$coefficients))
res <- do.call('cbind', estimates)
colnames(res) <- paste0('rho=', rhos)
t(res)

# true values in red
plot(y=res['x',],x=rhos,type='l', main = 'Coefficient on X', ylim = c(2.9,4))
abline(h=3, col = 'red')

```

--> OLS is biased for high rhos.



```