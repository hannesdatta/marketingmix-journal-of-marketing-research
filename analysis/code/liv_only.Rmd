---
title: "Parameter Recovery with Latent Instrumental Variables"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)
library(data.table)
library(parallel)
library(stats4)
library(optimx)
library(REndo)
max_reps = 100
n_cpu = 10

```

# 1) Overview

The purpose of this document is to develop and test code to recover parameters from a model with multiple endogeneous variables, using instrumental variables. The core methodology is explained in Ebbes et al. (2005) and Ebbes et al. (2009), and I borrow code from the `REndo` package to compare parameter recovery between (simple) versions of my own estimator with the recovery properties of `REndo`.

I proceed in the following steps:

In section 2, I simulate data. In section 3, I compare parameter recovery between `REndo` and my own estimator for cases in which only *one endogenous regressor* with *two classes* is used (this is what currently is supported by `REndo`). In section 4, I propose novel extensions to the LIV estimator, including the support for multiple endogenous regressors, multiple classes, and an arbitrary number of control variables. Finally, in section 5, I test the integrated complete likelihood (ICL) criterion for selection of the number of classes.


# 2) Simulation of data

```{r}

sim_data <- function(rho = .3,
                     N = 100,
                     beta_controls = NULL,
                     beta_intercept = 1,
                     latent_instruments = list(list(
                       pi = c(-1, 2),
                       p = c(.7, .3),
                       beta = .5
                     ))) {
  
  # Correlation matrix
  n=length(latent_instruments)+1
  R = diag(n)
  R[-1,1]<-rho
  R[1,-1]<-rho
  
  # Standard deviations
  S <- rep(sqrt(.5), n)
  
  cor2cov <- function(R, S) {
   sweep(sweep(R, 1, S, "*"), 2, S, "*")
  }
  
  Sigma = cor2cov(R, S)
  
  # Draws from the Var-Covar Matrix
  library(MASS)
  errorterm = mvrnorm(n = N, mu = rep(0, n), Sigma = Sigma)
  
  # Compute control variables (uniformly distributed)
  
  X_controls = matrix(runif(N*length(beta_controls)),ncol=length(beta_controls))
  if(nrow(X_controls)>0) colnames(X_controls) <- paste0('ctrl_', 1:length(beta_controls))
  
  instr = do.call('cbind', lapply(latent_instruments, function(liv) {
    # Assert the probs sum to 1
    stopifnot(sum(liv$p)==1)
  
    # Number of latent classes
    k = length(liv$pi)
  
    # Draw class memberships
    z <- sample(1:k, N, replace = T, prob = liv$p)
    
    #cbind(class = 1:k, frequency = table(z), prob = table(z)/sum(table(z)))

    # Simulate data
    x <- liv$pi[z]
    return(x)
    }))

  instr = instr+errorterm[,-1]
  
  colnames(instr) <- paste0('x', 1:length(latent_instruments))
  
  liv_betas = unlist(lapply(latent_instruments, function(x) x$beta))
  
  if(!is.null(beta_controls)) add_controls = X_controls %*% cbind(beta_controls) else add_controls = 0
  
  y = beta_intercept + instr%*%cbind(liv_betas) + add_controls + errorterm[,1]
  
  colnames(y) <- 'y'
  if (!is.null(beta_controls)) df_return = data.frame(y, instr, X_controls) else df_return = data.frame(y, instr)
  
  return(list(data=df_return,rho, N, latent_instruments))

}
```

# 3) Parameter recovery for one endogenous regressor

## OLS versus `REndo`-based estimator

```{r message=FALSE, warning=FALSE, include=FALSE}
# Startup cluster
init <- function() {
  library(REndo)
  library(data.table)
}
cl <- makePSOCKcluster(n_cpu)
clusterExport(cl, 'init')

```

```{r message=FALSE, warning=FALSE}
set.seed(1984)

reps = max_reps #100 # number of replications
data=lapply(1:reps, function(x) sim_data(N=120, rho = .6))

void <- clusterEvalQ(cl, init())

out = clusterApplyLB(cl, data, function(dt) {
  ols = lm(y ~ 1 + x1, data = dt$data)
  liv <- latentIV(y ~ x1, data = dt$data, optimx.args=list(itnmax=1E5, method='Nelder-Mead'))
  
                  
  return(cbind(
    ols = ols$coefficients[2],
    liv = liv$coefficients[2]
  ))
})


res = do.call('rbind',out)

kable(apply(res, 2, function(x) return(c(mean=mean(x), sd=sd(x)))), caption = 'Comparison between OLS versus REndo-based LIV estimator')

hist(drop(res[,2]),breaks=20, main = 'Histogram of bias-corrected parameter estimates')

```

## `REndo`-based estimator versus self-coded estimator

### One-case only, two levels, one regressor

```{r}
set.seed(1984)
out = sim_data(N=120, rho = .6)

# Load estimation likelihood function
source('liv_llik.R')

y=out$data$y
x=out$data$x1

# starting parameters
params = c(0, 0, 0, 1, 0, 1, 0, 0)

# without simulated maximum likelihood
MLE = optimx(params, fn = llik, method = 'Nelder-Mead', hessian = FALSE, itnmax = 10000, 
        control = list(trace = 0, dowarn = FALSE), levels = 2, endogenous_variables=1,
        data = list(y=y,endog=cbind(x),X=cbind(rep(1,length(y)))))

nlminb  = nlminb(start = params, objective = llik, levels = 2, endogenous_variables=1,
        data = list(y=y,endog=cbind(x),X=cbind(rep(1,length(y)))))

ols = lm(y~1+x)

kable(data.frame(ols=ols$coefficients, MLE_neldermead=unlist(MLE)[8:7],
           MLE_port=unlist(nlminb$par)[8:7]),
      caption = 'Comparison of parameter estimates between OLS and MLE estimators (Nelder-Mead, PORT)')

```

--> `nlminb` looks much more stable.

### Across `reps` replications

```{r} 
set.seed(1984)
reps = max_reps #100
data =lapply(1:reps, function(x) sim_data(N=120, rho = .6))

clusterExport(cl, c('llik', 'map_pars'))
void <- clusterEvalQ(cl, library(optimx))
void <- clusterEvalQ(cl, library(mvtnorm))
void <- clusterEvalQ(cl, library(ucminf))
void <- clusterEvalQ(cl, source('liv_llik.R'))

out = clusterApplyLB(cl, data, function(dt) {
  x<<-dt$data$x
  y<<-dt$data$y
  
  # get starting parameters
  
  #startpar = lm(y~1+x)$coefficients
  #cov=diag(2)
  #uppertri=cov[upper.tri(cov, diag = T)]
  #k=2
  #prob=1/(k)
  #pgo=log(prob/(1-prob))
  #lambdas = rep(0,2)
  #params = c(startpar, uppertri,pgo,lambdas)
  
  params = c(0, 0, 0, 1, 0, 1, 0, 0)

  nlminb  = nlminb(start = params, objective = llik, levels = 2, endogenous_variables=1,
        data = list(y=y,endog=cbind(x),X=cbind(rep(1,length(y)))))
  
  c(nlminb$par, nlminb$convergence, nlminb$objective)
  
})


res = do.call('rbind',out)

kable(t(apply(res, 2, function(x) return(c(mean=mean(x), sd=sd(x))))), caption = paste0('Parameter recovery across ', reps, ' replications'))

hist(drop(res[,7]),breaks=10, main = 'Histogram of bias-corrected parameter estimates')

```

# 4) Extensions

## 4.1) Multiple levels (one regressor)

```{r}
set.seed(1984)
# use lage sample size to obtain parameters close to the true values for benchmark purposes

out = sim_data(N=1200, rho = .3, latent_instruments = list(list(pi=c(-1,1,2),
                                                               p=c(.3,.4,.3),
                                                               beta=.5)))

# Load estimation likelihood function
source('liv_llik.R')

y=out$data$y
x=out$data$x1
X=cbind(rep(1, length(y)))
        
# starting parameters
params = c(0, 0, 0, -.5, 0, 1, 0, 1,  0, 0)
map_pars(params,levels=3, endogenous_variables = 1)
#map_pars(1:20,levels=3, endogenous_variables = 1)


nlminb  = nlminb(start = params, objective = llik, levels = 3, endogenous_variables=1,
        data = list(y=y,endog=cbind(x),X=X))
  

nlminb_old  = nlminb(start = params, objective = llik_old, levels = 3)

  
kable(data.frame(old=nlminb_old$par, new=nlminb$par),
      caption = 'Comparison of parameter estimates between REndo-inspired estimator and more flexible one that allows mixing distributions')

```

## 4.2) Two endogenous variables, with three levels

```{r}
set.seed(1984)
out = sim_data(N=1200, rho = .3, latent_instruments = list(list(pi=c(-1,1,2),
                                                               p=c(.3,.4,.3),
                                                               beta=.5),
                                                           list(pi=c(-2,0,1),
                                                               p=c(.1,.5,.4),
                                                               beta=-.5)))

# Load estimation likelihood function
source('liv_llik.R')

data = list(y=out$data$y, endog=cbind(out$data$x1, out$data$x2), X=cbind(rep(1,length(out$data$y))))
 
# starting parameters
params = c(0, 0, 0, 0, 0, 0, # lambdas
           0, 0, 0, 0, # prob
           1, 0, 1, 0, 0, 1, # chol
           0, 0, # gamma
           0) # beta

# try calling loglik

nlminb  = nlminb(
  start = params,
  objective = llik,
  levels = 3,
  endogenous_variables = 2,
  data = data,
  control = list(iter.max = 1000, eval.max = 1000)
)

map_pars(nlminb$par,
         levels = 3,
         endogenous_variables = 2)

```

## 4.3) Two instruments, three levels with control variables + intercept different from 1

```{r}
set.seed(1984)
out = sim_data(N=1200, rho = .3, beta_intercept = 2, beta_controls = c(-1,-2,1),
               latent_instruments = list(list(pi=c(-1,1,2),
                                                               p=c(.3,.4,.3),
                                                               beta=.5),
                                                           list(pi=c(-2,0,1),
                                                               p=c(.1,.5,.4),
                                                               beta=-.5)))

# Load estimation likelihood function
source('liv_llik.R')

ctrl_vars = out$data[, grepl('^ctrl', colnames(out$data))]


data = list(y=out$data$y, endog=cbind(out$data$x1, out$data$x2), X=as.matrix(cbind(rep(1,length(out$data$y)), ctrl_vars)))
 
# starting parameters
params = c(0, 0, 0, 0, 0, 0, # lambdas
           0, 0, 0, 0, # prob
           1, 0, 1, 0, 0, 1, # chol
           0, 0, # gamma
           0, 0, 0, 0) # beta (first one is intercept)

# try parameter mapping
map_pars(params,
         levels = 3,
         endogenous_variables = 2)

nlminb  = nlminb(
  start = params,
  objective = llik,
  levels = 3,
  endogenous_variables = 2,
  data = data,
  control = list(iter.max = 1000, eval.max = 1000)
)

map_pars(nlminb$par,
         levels = 3,
         endogenous_variables = 2)

```

# 5) Determining number of classes (using ICL)

## 5.1) Trying to recover three classes, for two endogenous regressors (without replication)


```{r}
set.seed(1984)

# true classes (three classes, two instruments)
out = sim_data(N=120, rho = .3, beta_intercept = 2, beta_controls = c(-1,-2,1),
               latent_instruments = list(list(pi=c(-1,1,2),
                                              p=c(.3,.4,.3),
                                              beta=.5),
                                         list(pi=c(-2,0,1),
                                              p=c(.1,.5,.4),
                                              beta=-.5)))

# Load estimation likelihood function
source('liv_llik.R')

ctrl_vars = out$data[, grepl('^ctrl', colnames(out$data))]
data = list(y=out$data$y, endog=cbind(out$data$x1, out$data$x2), X=as.matrix(cbind(rep(1,length(out$data$y)), ctrl_vars)))

# assemble starting parameters
results <- sapply(2:4, function(levels) {
  cat('Estimationg for ', levels, ' levels...\n')
  #levels = 3
  endogenous_variables = 2
   
  # starting parameters
  params = c(rep(0, levels * endogenous_variables), #0, 0, 0, 0, 0, 0, # lambdas
             rep(0, (levels-1)*endogenous_variables), #           0, 0, 0, 0, # prob
             1, 0, 1, 0, 0, 1, # chol
             0, 0, # gamma
             0, 0, 0, 0) # beta (first one is intercept)
  
  # try parameter mapping
  map_pars(params,
           levels = levels,
           endogenous_variables = endogenous_variables)
  
  nlminb  = nlminb(
    start = params,
    objective = llik,
    levels = levels,
    endogenous_variables = endogenous_variables,
    data = data,
    control = list(iter.max = 1000, eval.max = 1000)
  )
  
  if(0) {
  # compare to lm
  summary(lm(y~x1+x2+ctrl_1+ctrl_2+ctrl_3, data=out$data))
  
  nlminb  = nlminb(
    start = params,
    objective = llik_lm,
    endogenous_variables = endogenous_variables,
    data = data,
    control = list(iter.max = 1000, eval.max = 1000)
  )
  
   map_pars(nlminb$par,
           levels = 1,
           endogenous_variables = endogenous_variables)
        
  }
  
  return(c(levels=levels, endogenous_variables=endogenous_variables, unlist(llik_complete(nlminb$par, levels = levels, endogenous_variables = endogenous_variables, data = data))))
})

results

```

## 5.2) Trying to recover `reps` classes, with just one endogenous regressor

```{r}
set.seed(1984)
reps = max_reps #10
data=lapply(1:reps, function(x) sim_data(N=120, rho = .3, beta_intercept = 2, beta_controls = c(-1,-2,1),
               latent_instruments = list(list(pi=c(-1,1,2),
                                              p=c(.2,.6,.2),
                                              beta=.5))))

clusterExport(cl, c('llik', 'llik_complete', 'map_pars'))

void <- clusterEvalQ(cl, library(mvtnorm))
void <- clusterEvalQ(cl, source('liv_llik.R'))

out = clusterApplyLB(cl, data, function(dt) {
  try({
  ctrl_vars = dt$data[, grepl('^ctrl', colnames(dt$data))]
  #endog_vars = dt$data[, grepl('^x[0-9]', colnames(dt$data))]
  
  my_data <<- list(y=dt$data$y, endog=cbind(dt$data$x1), X=as.matrix(cbind(rep(1,length(dt$data$y)), ctrl_vars)))

  results <- sapply(2:4, function(levels) {
    cat('Estimating for ', levels, ' levels...\n')
    #levels = 3
    endogenous_variables = 1
   
    # starting parameters
    params = c(rep(0, levels * endogenous_variables), #0, 0, 0, 0, 0, 0, # lambdas
               rep(0, (levels-1)*endogenous_variables), #           0, 0, 0, 0, # prob
               chol(diag(endogenous_variables+1))[upper.tri(diag(endogenous_variables+1),diag=T)],# chol
               rep(0, endogenous_variables), # gamma
               0, 0, 0, 0) # beta (first one is intercept)
    
  nlminb  = nlminb(
    start = params,
    objective = llik,
    levels = levels,
    endogenous_variables = endogenous_variables,
    data = my_data,
    control = list(iter.max = 1000, eval.max = 1000)
  )
  
  return(c(levels=levels, endogenous_variables=endogenous_variables, unlist(llik_complete(nlminb$par, levels = levels, endogenous_variables = endogenous_variables, data = my_data))))
  })
  
  t(results)
  }, silent=T)
})


```


```{r}
iter=0
results=rbindlist(lapply(seq(along=out), function(i) {
  iter = iter + 1
  data.table(model = i, out[[i]])
}))

res=results[, list(choice_icl = levels[icl==min(icl)],
               choice_bic = levels[bic==min(bic)]), by = c('model')]
table(res$choice_icl)
table(res$choice_bic)
```


## 5.3) Trying to recover `reps` classes, with two endogenous regressors

```{r}
set.seed(1984)
reps = max_reps #10
data=lapply(1:reps, function(x) sim_data(N=120, rho = .3, beta_intercept = 2, beta_controls = c(-1,-2,1),
               latent_instruments = list(list(pi=c(-1,1,2),
                                              p=c(.3,.4,.3),
                                              beta=.5),
                                         list(pi=c(-2,0,1),
                                              p=c(.1,.5,.4),
                                              beta=-.5))))

clusterExport(cl, c('llik', 'llik_complete', 'map_pars'))

void <- clusterEvalQ(cl, library(mvtnorm))
void <- clusterEvalQ(cl, source('liv_llik.R'))

out = clusterApplyLB(cl, data, function(dt) {
  
  ctrl_vars = dt$data[, grepl('^ctrl', colnames(dt$data))]
  #endog_vars = dt$data[, grepl('^x[0-9]', colnames(dt$data))]
  
  my_data <<- list(y=dt$data$y, endog=cbind(dt$data$x1, dt$data$x2), X=as.matrix(cbind(rep(1,length(dt$data$y)), ctrl_vars)))

  results <- sapply(2:4, function(levels) {
    cat('Estimating for ', levels, ' levels...\n')
    #levels = 3
    endogenous_variables = 2
   
    # starting parameters
    params = c(rep(0, levels * endogenous_variables), #0, 0, 0, 0, 0, 0, # lambdas
               rep(0, (levels-1)*endogenous_variables), #           0, 0, 0, 0, # prob
               chol(diag(endogenous_variables+1))[upper.tri(diag(endogenous_variables+1),diag=T)],# chol
               rep(0, endogenous_variables), # gamma
               0, 0, 0, 0) # beta (first one is intercept)
    
  nlminb  = nlminb(
    start = params,
    objective = llik,
    levels = levels,
    endogenous_variables = endogenous_variables,
    data = my_data,
    control = list(iter.max = 1000, eval.max = 1000)
  )
  
  return(c(levels=levels, endogenous_variables=endogenous_variables, unlist(llik_complete(nlminb$par, levels = levels, endogenous_variables = endogenous_variables, data = my_data))))
  })
  
  t(results)

})


iter=0
results=rbindlist(lapply(seq(along=out), function(i) {
  iter = iter + 1
  data.table(model = i, out[[i]])
}))

res=results[, list(choice_icl = levels[icl==min(icl)],
               choice_bic = levels[bic==min(bic)]), by = c('model')]
kable(table(res$choice_icl), caption = "Number of classes chosen with ICL")
kable(table(res$choice_bic), caption = "Number of classes chosen with BIC")
```

# References

Biernacki, C., Celeux, G., & Govaert, G. (2000). Assessing a mixture model for clustering with the integrated completed likelihood. IEEE transactions on pattern analysis and machine intelligence, 22(7), 719-725. https://doi.org/10.1109/34.865189. --> original reference for ICL criterion

Ebbes, P., Wedel, M., Böckenholt, U. et al. (2005). Solving and Testing for Regressor-Error (in)Dependence When no Instrumental Variables are Available: With New Evidence for the Effect of Education on Income. *Quantitative Marketing and Economics*, 3(4), 365–392. https://doi.org/10.1007/s11129-005-1177-6 --> original LIV paper

Ebbes, P., Wedel, M., & Böckenholt, U. (2009). Frugal IV alternatives to identify the parameter for an endogenous regressor. Journal of Applied Econometrics, 24(3), 446-468. https://doi.org/10.1002/jae.1058. --> paper underlying `REndo`

https://rdrr.io/cran/rebmix/ --> ICL implementation in R, but not with LIV


# Next steps
- Model selection
  - Verify correct implementation of ICL (e.g., correct classes)
  - Can BIC be ok, too?
  
- Varying classes per regressor

- Move code into a package that can easily be used
