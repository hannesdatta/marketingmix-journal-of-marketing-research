---
title: "Simulation of Latent Instrumental Variables"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r, echo = FALSE}
library(knitr)
library(data.table)
library(parallel)
library(stats4)
library(optimx)
```

As a first step towards our own LIV estimation for the GfK project, I propose to code up:

Ebbes, P., Wedel, M., BÃ¶ckenholt, U., & Steerneman, T. (2005). Solving and testing for regressor-error (in) dependence when no instrumental variables are available: With new evidence for the effect of education on income. *Quantitative Marketing and Economics*, 3(4), 365-392. To the extent possible, I adhere to the notation in that paper.

# Simulation of data

```{r}


sim_data <- function(rho = .3,
                     N = 100,
                     latent_instruments = list(list(
                       pi = c(-1, 2),
                       p = c(.7, .3),
                       beta = .5
                     ))) {
  
  # Correlation matrix
  n=length(latent_instruments)+1
  R = diag(n)
  R[-1,1]<-rho
  R[1,-1]<-rho
  
  # Standard deviations
  S <- rep(sqrt(.5), n)
  
  cor2cov <- function(R, S) {
   sweep(sweep(R, 1, S, "*"), 2, S, "*")
  }
  
  Sigma = cor2cov(R, S)
  
  # Draws from the Var-Covar Matrix
  library(MASS)
  errorterm = mvrnorm(n = N, mu = rep(0, n), Sigma = Sigma)
  
  #Compute endogenous regressor (uniformly distributed from min(unif) to max(unif)
  #cor(errorterm)
  #cov(errorterm)

  instr = do.call('cbind', lapply(latent_instruments, function(liv) {
    # Assert the probs sum to 1
    stopifnot(sum(liv$p)==1)
  
    # Number of latent classes
    k = length(liv$pi)
  
    # Draw class memberships
    z <- sample(1:k, N, replace = T, prob = liv$p)
    
    #cbind(class = 1:k, frequency = table(z), prob = table(z)/sum(table(z)))

    # Simulate data
    x <- liv$pi[z]
    return(x)
    }))

  instr = instr+errorterm[,-1]
  
  colnames(instr) <- paste0('x', 1:length(latent_instruments))
  
  liv_betas = unlist(lapply(latent_instruments, function(x) x$beta))
  
  
  
  y = 1 + instr%*%cbind(liv_betas) + errorterm[,1]
  
  colnames(y) <- 'y'
  return(list(data=data.frame(y,instr),rho, N, latent_instruments))

}
```

# Parameter recovery for one endogenous regressor

## OLS versus `REndo`-based estimator

```{r include=FALSE}
# Startup cluster
init <- function() {
  library(REndo)
  library(data.table)
}
cl <- makePSOCKcluster(4)
clusterExport(cl, 'init')

```

```{r message=FALSE, warning=FALSE}
set.seed(1984)

reps = 100 # number of replications
data=lapply(1:reps, function(x) sim_data(N=120, rho = .6))

void <- clusterEvalQ(cl, init())

out = clusterApplyLB(cl, data, function(dt) {
  ols = lm(y ~ 1 + x1, data = dt$data)
  liv <- latentIV(y ~ x1, data = dt$data, optimx.args=list(itnmax=1E5, method='Nelder-Mead'))
  
                  
  return(cbind(
    ols = ols$coefficients[2],
    liv = liv$coefficients[2]
  ))
})


res = do.call('rbind',out)

kable(apply(res, 2, function(x) return(c(mean=mean(x), sd=sd(x)))), caption = 'Comparison between OLS versus REndo-based LIV estimator')

hist(drop(res[,2]),breaks=20, main = 'Histogram of bias-corrected parameter estimates')

```

## `REndo`-based estimator versus self-coded estimator

### One-case only, two levels, one regressor

```{r}
set.seed(1984)
out = sim_data(N=120, rho = .6)

# Load estimation likelihood function
source('liv_llik.R')

y=out$data$y
x=out$data$x1

# starting parameters
params = c(0, 1, 1, 0, 1, 0, 1, 0)

# without simulated maximum likelihood
MLE = optimx(params, fn = llik, method = 'Nelder-Mead', hessian = FALSE, itnmax = 10000, 
        control = list(trace = 0, dowarn = FALSE))

nlminb  = nlminb(start = params, objective = llik)

ols = lm(y~1+x)

kable(data.frame(ols=ols$coefficients, MLE_neldermead=unlist(MLE)[1:2],
           MLE_port=unlist(nlminb$par)[1:2]),
      caption = 'Comparison of parameter estimates between OLS and MLE estimators (Nelder-Mead, PORT)')

```


### Across `reps` replications

```{r} 
set.seed(1984)
reps = 100
data=lapply(1:reps, function(x) sim_data(N=120, rho = .6))

clusterExport(cl, c('llik', 'map_pars'))
clusterEvalQ(cl, library(optimx))
clusterEvalQ(cl, library(mvtnorm))
clusterEvalQ(cl, library(ucminf))

clusterEvalQ(cl, source('liv_llik.R'))
#void<-clusterEvalQ(cl, {simvals <<- matrix(runif(N*reps),ncol=100)})



out = clusterApplyLB(cl, data, function(dt) {
  x<<-dt$data$x
  y<<-dt$data$y
  
  # get starting parameters
  startpar = lm(y~1+x)$coefficients
  cov=diag(2)
  uppertri=cov[upper.tri(cov, diag = T)]
  k=2
  prob=1/(k)
  pgo=log(prob/(1-prob))
  lambdas = rep(0,2)
  
  #params = c(0, 1, 1, 0, 1, 0, 1, 2)
  
  #params = c(0, 0, 1, 0, 1, 0, 1, 0)
  
  #params = c(0, 0, 1, 0, 1, 0, 1, -10)
  #params = c(0, 0, 1, 0, 1, 0, 0, -10)
  #pgo = -1
  #lambdas=c(-1,1)
  params = c(startpar, uppertri,pgo,lambdas)
  #params = c(startpar, uppertri,0,0,0)
  
  #ucminf  = ucminf::ucminf(par = params, fn = llik, hessian = 0, sim = T, reparametrization = T)
  #llik(c(startpar, uppertri,-1,lambdas), sim = T, reparametrization = T)
  
  nlminb  = nlminb(start = params, objective = llik)#,
  
  #MLE = optimx(params, fn = llik, method = 'Nelder-Mead', hessian = FALSE, itnmax = 10000, 
  #      control = list(trace = 0, dowarn = FALSE))#, sim = TRUE,reparametrization = FALSE)
  
  #unlist(MLE)
  c(nlminb$par, nlminb$convergence, nlminb$objective)
  
})


res = do.call('rbind',out)

kable(t(apply(res, 2, function(x) return(c(mean=mean(x), sd=sd(x))))), caption = paste0('Parameter recovery across ', reps, ' replications'))

hist(drop(res[,2]),breaks=10, main = 'Histogram of bias-corrected parameter estimates')

```

### One-case only, three levels, one regressor

```{r}
set.seed(1984)
out = sim_data(N=1200, rho = .3, latent_instruments = list(list(pi=c(-1,1,2),
                                                               p=c(.3,.4,.3),
                                                               beta=.5)))

# Load estimation likelihood function
source('liv_llik.R')

y=out$data$y
x=out$data$x1
X=cbind(rep(1, length(y)))
        
# starting parameters
params = c(0, 0, 0, -.5, 0, 1, 0, 1,  0, 0)
map_pars(params,levels=3, endogenous_variables = 1)


# without simulated maximum likelihood
MLE = optimx(params, fn = llik, method = 'Nelder-Mead', hessian = FALSE, 
             itnmax = 10000, 
        control = list(trace = 0, dowarn = FALSE), levels=3, endogenous_variables=1, data = list(y=y,endog=cbind(x),
                                                                                                                   X=X))

#map_pars(unlist(MLE)[1:10], levels=3, endogenous_variables = 1)


#llik(params, levels=3,endogenous_variables = 1, data = list(y=y,X=cbind(rep(1, length(y))), endog=cbind(x)))


nlminb  = nlminb(start = params, objective = llik, levels = 3,
                 endogenous_variables=1, data=list(y=y,endog=cbind(x),
                                                                                                                   X=X))
#map_pars(nlminb$par, levels=3, endogenous_variables = 1)



ols = lm(y~1+x)

kable(data.frame(ols=ols$coefficients[2], MLE_neldermead=unlist(MLE)[7],
           MLE_port=unlist(nlminb$par)[7]),
      caption = 'Comparison of parameter estimates between OLS and MLE estimators (Nelder-Mead, PORT)')

#map_pars(nlminb$par, levels=3)

```


# Steps:

# Recovered three-level LIV with CLOSED FORM solution!

# Only thing that remains: extending to multiple-instrument case + adding other controls to equation!



```


