---
title: "Simulation of Latent Instrumental Variables"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r, echo = FALSE}
library(knitr)
library(data.table)
library(parallel)
library(stats4)
library(optimx)
```

As a first step towards our own LIV estimation for the GfK project, I propose to code up:

Ebbes, P., Wedel, M., BÃ¶ckenholt, U., & Steerneman, T. (2005). Solving and testing for regressor-error (in) dependence when no instrumental variables are available: With new evidence for the effect of education on income. *Quantitative Marketing and Economics*, 3(4), 365-392. To the extent possible, I adhere to the notation in that paper.

# Simulation of data

```{r}

sim_data <- function(rho = .3,
                     N = 100,
                     latent_instruments = list(list(
                       pi = c(-1, 2),
                       p = c(.7, .3),
                       beta = .5
                     ))) {
  
  # Correlation matrix
  n=length(latent_instruments)+1
  R = diag(n)
  R[-1,1]<-rho
  R[1,-1]<-rho
  
  # Standard deviations
  S <- rep(sqrt(.5), n)
  
  cor2cov <- function(R, S) {
   sweep(sweep(R, 1, S, "*"), 2, S, "*")
  }
  
  Sigma = cor2cov(R, S)
  
  # Draws from the Var-Covar Matrix
  library(MASS)
  errorterm = mvrnorm(n = N, mu = rep(0, n), Sigma = Sigma)
  
  #Compute endogenous regressor (uniformly distributed from min(unif) to max(unif)
  #cor(errorterm)
  #cov(errorterm)

  instr = do.call('cbind', lapply(latent_instruments, function(liv) {
    # Assert the probs sum to 1
    stopifnot(sum(liv$p)==1)
  
    # Number of latent classes
    k = length(liv$pi)
  
    # Draw class memberships
    z <- sample(1:k, N, replace = T, prob = liv$p)
    
    #cbind(class = 1:k, frequency = table(z), prob = table(z)/sum(table(z)))

    # Simulate data
    x <- liv$pi[z]
    return(x)
    }))

  instr = instr+errorterm[,-1]
  
  colnames(instr) <- paste0('x', 1:length(latent_instruments))
  
  liv_betas = unlist(lapply(latent_instruments, function(x) x$beta))
  
  
  
  y = 1 + instr%*%cbind(liv_betas) + errorterm[,1]
  
  colnames(y) <- 'y'
  return(list(data=data.frame(y,instr),rho, N, latent_instruments))

}
```

# Parameter recovery for one endogenous regressor

## OLS versus `REndo`-based estimator

```{r include=FALSE}
# Startup cluster
init <- function() {
  library(REndo)
  library(data.table)
}
cl <- makePSOCKcluster(4)
clusterExport(cl, 'init')

```

```{r message=FALSE, warning=FALSE}
set.seed(1984)

reps = 100 # number of replications
data=lapply(1:reps, function(x) sim_data(N=120, rho = .6))

void <- clusterEvalQ(cl, init())

out = clusterApplyLB(cl, data, function(dt) {
  ols = lm(y ~ 1 + x1, data = dt$data)
  liv <- latentIV(y ~ x1, data = dt$data, optimx.args=list(itnmax=1E5, method='Nelder-Mead'))
  
                  
  return(cbind(
    ols = ols$coefficients[2],
    liv = liv$coefficients[2]
  ))
})


res = do.call('rbind',out)

kable(apply(res, 2, function(x) return(c(mean=mean(x), sd=sd(x)))), caption = 'Comparison between OLS versus REndo-based LIV estimator')

hist(drop(res[,2]),breaks=20, main = 'Histogram of bias-corrected parameter estimates')

```

## `REndo`-based estimator versus self-coded estimator

### One-case only, two levels, one regressor

```{r}
set.seed(1984)
out = sim_data(N=120, rho = .6)

# Load estimation likelihood function
source('liv_llik.R')

y=out$data$y
x=out$data$x1

# starting parameters
params = c(0, 0, 0, 1, 0, 1, 0, 0)

# without simulated maximum likelihood
MLE = optimx(params, fn = llik, method = 'Nelder-Mead', hessian = FALSE, itnmax = 10000, 
        control = list(trace = 0, dowarn = FALSE), levels = 2, endogenous_variables=1,
        data = list(y=y,endog=cbind(x),X=cbind(rep(1,length(y)))))

nlminb  = nlminb(start = params, objective = llik, levels = 2, endogenous_variables=1,
        data = list(y=y,endog=cbind(x),X=cbind(rep(1,length(y)))))

ols = lm(y~1+x)

kable(data.frame(ols=ols$coefficients, MLE_neldermead=unlist(MLE)[8:7],
           MLE_port=unlist(nlminb$par)[8:7]),
      caption = 'Comparison of parameter estimates between OLS and MLE estimators (Nelder-Mead, PORT)')

```

--> `nlminb` looks much more stable.

### Across `reps` replications

```{r} 
set.seed(1984)
reps = 100
data=lapply(1:reps, function(x) sim_data(N=120, rho = .6))

clusterExport(cl, c('llik', 'map_pars'))
void <- clusterEvalQ(cl, library(optimx))
void <- clusterEvalQ(cl, library(mvtnorm))
void <- clusterEvalQ(cl, library(ucminf))
void <- clusterEvalQ(cl, source('liv_llik.R'))

out = clusterApplyLB(cl, data, function(dt) {
  x<<-dt$data$x
  y<<-dt$data$y
  
  # get starting parameters
  
  #startpar = lm(y~1+x)$coefficients
  #cov=diag(2)
  #uppertri=cov[upper.tri(cov, diag = T)]
  #k=2
  #prob=1/(k)
  #pgo=log(prob/(1-prob))
  #lambdas = rep(0,2)
  #params = c(startpar, uppertri,pgo,lambdas)
  
  params = c(0, 0, 0, 1, 0, 1, 0, 0)

  nlminb  = nlminb(start = params, objective = llik, levels = 2, endogenous_variables=1,
        data = list(y=y,endog=cbind(x),X=cbind(rep(1,length(y)))))
  
  c(nlminb$par, nlminb$convergence, nlminb$objective)
  
})


res = do.call('rbind',out)

kable(t(apply(res, 2, function(x) return(c(mean=mean(x), sd=sd(x))))), caption = paste0('Parameter recovery across ', reps, ' replications'))

hist(drop(res[,7]),breaks=10, main = 'Histogram of bias-corrected parameter estimates')

```

### One-case only, three levels, one regressor

```{r}
set.seed(1984)
# use lage sample size to obtain parameters close to the true values for benchmark purposes

out = sim_data(N=1200, rho = .3, latent_instruments = list(list(pi=c(-1,1,2),
                                                               p=c(.3,.4,.3),
                                                               beta=.5)))

# Load estimation likelihood function
source('liv_llik.R')

y=out$data$y
x=out$data$x1
X=cbind(rep(1, length(y)))
        
# starting parameters
params = c(0, 0, 0, -.5, 0, 1, 0, 1,  0, 0)
map_pars(params,levels=3, endogenous_variables = 1)
#map_pars(1:20,levels=3, endogenous_variables = 1)


nlminb  = nlminb(start = params, objective = llik, levels = 3, endogenous_variables=1,
        data = list(y=y,endog=cbind(x),X=X))
  

nlminb_old  = nlminb(start = params, objective = llik_old, levels = 3)

  
kable(data.frame(old=nlminb_old$par, new=nlminb$par),
      caption = 'Comparison of parameter estimates between REndo-inspired estimator and more flexible one that allows mixing distributions')

```

### Two instruments, three levels

```{r}
set.seed(1984)
out = sim_data(N=1200, rho = .3, latent_instruments = list(list(pi=c(-1,1,2),
                                                               p=c(.3,.4,.3),
                                                               beta=.5),
                                                           list(pi=c(-2,0,1),
                                                               p=c(.1,.5,.4),
                                                               beta=-.5)))

# Load estimation likelihood function
source('liv_llik.R')

data = list(y=out$data$y, endog=cbind(out$data$x1, out$data$x2), X=cbind(rep(1,length(out$data$y))))
 
# starting parameters
params = c(0, 0, 0, 0, 0, 0, # lambdas
           0, 0, 0, 0, # prob
           1, 0, 1, 0, 0, 1, # chol
           0, 0, # gamma
           0) # beta

# try calling loglik

nlminb  = nlminb(start = params, objective = llik, levels = 3, endogenous_variables=2,
        data = data, control = list(iter.max=1000, eval.max = 1000))


map_pars(nlminb$par, levels = 3, endogenous_variables=2)

```

# Steps to move this into production:
- Add "noise" to estimator in the form of other regression data and check recovery attributes
- Check whether correct number of classes can be recovered (e.g., some kind of AIC, BIC to be supplied in the model results)
- Evaluate recovery with *small* N (=120) and 100 replications (as above), including number of classes

- Move code into a package that can easily be used

